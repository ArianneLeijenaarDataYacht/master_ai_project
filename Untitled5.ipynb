{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TRAIN LSTM Pickle Hourly incl knmi.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-467ec5543c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TRAIN LSTM Pickle Hourly incl knmi.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TEST LSTM Pickle Hourly incl knmi.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0msplit_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TRAIN LSTM Pickle Hourly incl knmi.pkl'"
     ]
    }
   ],
   "source": [
    "# prepare data for lstm\n",
    "from pandas import read_pickle\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "import numpy as np\n",
    "import pandas as pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "desired_width=320\n",
    "\n",
    "pandas.set_option('display.width', desired_width)\n",
    "pandas.set_option('display.max_columns',7)\n",
    "\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def plot_dataset(values,dataset):\n",
    "\tgroups = [0,1,2,3,4]\n",
    "\ti = 1\n",
    "\tpyplot.figure()\n",
    "\tfor group in groups:\n",
    "\t\tpyplot.subplot(len(groups), 1, i)\n",
    "\t\tpyplot.plot(values[:, group])\n",
    "\t\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
    "\t\ti += 1\n",
    "\tpyplot.show()\n",
    "\n",
    "\n",
    "def split_data(dataset):\n",
    "\t# split into train and test sets\n",
    "\tvalues = dataset.values\n",
    "\ttrain = values[:split_point, :]\n",
    "\ttest = values[split_point:, :]\n",
    "\n",
    "\treturn train,test\n",
    "\n",
    "\n",
    "def input_output(dataset,features,time_steps_lag, time_steps):\n",
    "\t# split into input and outputs\n",
    "\ttrain, test = split_data(dataset)\n",
    "\tn_obs = features * time_steps_lag\n",
    "\ttrain_X, train_y = train[:, :n_obs], train[:,n_obs+features*(time_steps+1)-1]\n",
    "\ttest_X, test_y = test[:, :n_obs], test[:, n_obs+features*(time_steps+1)-1]\n",
    "\t\n",
    "\treturn train_X, train_y, test_X, test_y, n_obs\n",
    "\n",
    "\n",
    "def reshape_input(train_X, test_X):\n",
    "\ttrain_X = train_X.reshape((train_X.shape[0], time_steps_lag, features))\n",
    "\ttest_X = test_X.reshape((test_X.shape[0], time_steps_lag, features))\n",
    "\treturn train_X , test_X\n",
    "\n",
    "\n",
    "def design_network(train_X,hiddenlayer1, hiddenlayer2,):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(GRU(hiddenlayer1, return_sequences = True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "\tmodel.add(GRU(hiddenlayer2))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def fit_network(model,batch, epoch, number):\n",
    "\thistory = model.fit(train_X, train_y, epochs=epoch, batch_size=batch,\n",
    "\t\t\t\t\t\tverbose=2, shuffle=False)\n",
    "\tif x == 1:\n",
    "\t\tax1.plot(history.history['loss'], label = 'Training')\n",
    "\t\tax1.set_title(f\"Loss of t+{number}\")\n",
    "\t\tax1.set(ylabel = \"MSE\",xlabel=\"Epochs\")\n",
    "\telse:\n",
    "\t\taxs[number-start_time_step,0].plot(history.history['loss'], label = 'Training')\n",
    "\t\taxs[number-start_time_step,0].set_title(f\"Loss of t+{number}\")\n",
    "\t\taxs[number-start_time_step,0].set(ylabel = \"MSE\")\n",
    "\t\tif number == last_time_step -1:\n",
    "\t\t\taxs[number - start_time_step, 0].set(xlabel=\"Epochs\")\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def create_prediction_graph(model, test_X, test_y, time_steps_lag, features):\n",
    "\tyhat = model.predict(test_X)\n",
    "\ttest_X = test_X.reshape((test_X.shape[0], time_steps_lag * features))\n",
    "\t# invert scaling for forecast\n",
    "\tinv_yhat = concatenate((test_X[:, -features:-1], yhat), axis=1)\n",
    "\tinv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "\tinv_yhat = inv_yhat[:, features-1]\n",
    "\t# invert scaling for actual\n",
    "\ttest_y = test_y.reshape((len(test_y), 1))\n",
    "\tinv_y = concatenate(( test_X[:, -features:-1], test_y,), axis=1)\n",
    "\tinv_y = scaler.inverse_transform(inv_y)\n",
    "\tinv_y = inv_y[:, features-1]\n",
    "\tmean_true_y = np.full((len(inv_y),1),inv_y.mean())\n",
    "\n",
    "\tNSE = 1 - mean_squared_error(inv_y, inv_yhat)/mean_squared_error(inv_y, mean_true_y)\n",
    "\trmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "\treturn  rmse, inv_y, inv_yhat , NSE\n",
    "\n",
    "def create_subplot(i,start_time_step,true_y,predict_y,rmse,NSE,last_time_step):\n",
    "\tif x == 1:\n",
    "\t\tax2.plot(true_y, label = \"True\", alpha = 1)\n",
    "\t\tax2.plot(predict_y, label = \"Prediction\", alpha = 0.7)\n",
    "\t\tax2.set_title(f\"Graph t+{i}  RMSE: {round(rmse, 4)} NSE: {round(NSE,4)}\")\n",
    "\t\tax2.set(xlabel = \"Instances\", ylabel = \"MNAP\")\n",
    "\telse:\n",
    "\t\taxs[i-start_time_step,1].plot(true_y, label = \"True\", alpha = 1)\n",
    "\t\taxs[i-start_time_step,1].plot(predict_y, label = \"Prediction\", alpha = 0.7)\n",
    "\t\taxs[i-start_time_step,1].set_title(f\"Graph t+{i}  RMSE: {round(rmse, 4)} NSE: {round(NSE,4)}\")\n",
    "\t\taxs[i - start_time_step, 1].set(ylabel=\"mNAP\")\n",
    "\t\tif i == last_time_step -1:\n",
    "\t\t\taxs[i - start_time_step, 1].set(xlabel = \"Instances\", ylabel = \"MNAP\")\n",
    "\n",
    "# load dataset\n",
    "train_data = read_pickle(\"C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TRAIN LSTM Pickle Hourly incl knmi.pkl\")\n",
    "test_data = read_pickle(\"C:/Users/thomo/OneDrive/Documenten/GitHub/waternet-overstort/data/TEST LSTM Pickle Hourly incl knmi.pkl\")\n",
    "split_point = len(train_data)\n",
    "dataset = pandas.concat([train_data, test_data])\n",
    "\n",
    "\n",
    "# Move feature to be predicted to end of dataset and use time as index column\n",
    "dataset = dataset.set_index(\"Time\")\n",
    "dataset2 = dataset.pop(\"BBB sewer\")\n",
    "result = pandas.concat([dataset, dataset2], axis =1)\n",
    "dataset = result\n",
    "\n",
    "\n",
    "\n",
    "# Drop features from dataset\n",
    "#dataset = dataset.drop(columns=[\"knmi1\",\"knmi2\",\"knmi3\",\"knmi4\",\"knmi5\",\"knmi6\",\"knmi7\"])\n",
    "values_train = train_data.values\n",
    "values_train = values_train.astype('float32')\n",
    "# ensure all data is float\n",
    "values = dataset.values\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit(values_train)\n",
    "scaled = scaled.transform(values)\n",
    "\n",
    "## initialize variables\n",
    "\n",
    "#How many time steps in the future the model should train on\n",
    "# 1 model trained per time step, so univariate output,\n",
    "# not 1 model trained on 6 time steps (multivariate output)\n",
    "# if last_time_step = 2, model for t and t+1 will be trained\n",
    "# range[1:]\n",
    "last_time_step = 6\n",
    "\n",
    "#From which time step in the future it will start training on range[0:last_time_step-1]\n",
    "\n",
    "start_time_step = 5\n",
    "\n",
    "#How many time steps in the past should be given as an extra feature range[1:]\n",
    "time_steps_lag = 6\n",
    "\n",
    "#Amount of input features including feature to be predicted\n",
    "features = len(dataset.columns)\n",
    "\n",
    "#output shape of GRU layer, terminology inconsistent can be called amount of units/neurons/hiddenlayers\n",
    "hiddenlayer1 = 320\n",
    "hiddenlayer2 = 128\n",
    "\n",
    "#How many instances the GRU will go through before it updates it's weights. Too small batch size leads to overfitting\n",
    "batch_size = 1000\n",
    "\n",
    "#How often to go through the whole dataset\n",
    "epoch = 10\n",
    "\n",
    "\n",
    "\n",
    "# create supervised dataset with past- and future time steps\n",
    "reframed = series_to_supervised(scaled, time_steps_lag, last_time_step)\n",
    "\n",
    "#option to plot whole dataset for visualization\n",
    "#plot_dataset(values, dataset)\n",
    "\n",
    "#Create figure with corresponding subplots. Can't have input of single timestep, will be fixed later\n",
    "x = last_time_step - start_time_step\n",
    "if x == 1:\n",
    "\tfigure, (ax1,ax2) = plt.subplots(1, 2, sharex= \"col\", sharey =\"col\", figsize=(7,4.5))\n",
    "else:\n",
    "\tfigure, axs = plt.subplots(x,2, sharex= \"col\", sharey =\"col\", figsize=(10,6.5))\n",
    "\n",
    "\n",
    "#train, fit, plot loss, plot prediction for every time step\n",
    "for i in range(start_time_step,last_time_step):\n",
    "\ttrain_X, train_y, test_X, test_y, n_obs = input_output(reframed, features, time_steps_lag, i)\n",
    "\ttrain_X, test_X = reshape_input(train_X, test_X)\n",
    "\tmodel = design_network(train_X,hiddenlayer1,hiddenlayer2)\n",
    "\tmodel = fit_network(model, batch_size, epoch,i)\n",
    "\trmse, true_y, predict_y, NSE = create_prediction_graph(model, test_X, test_y, time_steps_lag, features)\n",
    "\t#compute in prediction subplot\n",
    "\tcreate_subplot(i,start_time_step,true_y,predict_y,rmse,NSE,last_time_step)\n",
    "\tprint('test RMSE: %.3f' % rmse)\n",
    "\tprint('test NSE: %.3f' % NSE)\n",
    "#\n",
    "if x == 1:\n",
    "\tax1.legend()\n",
    "\tax1.legend()\n",
    "else:\n",
    "\taxs[0, 0].legend()\n",
    "\taxs[0, 1].legend()\n",
    "figure.suptitle(\"Prediction and Loss Including KNMI-forecast\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
